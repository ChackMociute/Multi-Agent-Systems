{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa28be61-e17f-4e9b-a4e0-bf6cc65f88d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import choices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a7b106-677a-4abc-9167-6adbc1a18d48",
   "metadata": {},
   "source": [
    "# Fictitious play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca8382b-9967-4673-a46f-fd5e4ff20ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, actions, rewards, probs=None):\n",
    "        self.best_response = False\n",
    "        self.actions = self.create_actions(actions, probs)\n",
    "        self.rewards = rewards\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_actions(actions, probs):\n",
    "        if probs is None:\n",
    "            return {a: 1/len(actions) for a in actions}\n",
    "        if len(actions) != len(probs):\n",
    "            raise RuntimeError(\"Actions and probabilities should have the same length\")\n",
    "        if sum(probs) != 1:\n",
    "            raise ValueError(\"Probabilities should add up to 1\")\n",
    "        return {a: p for a, p in zip(actions, probs)}\n",
    "    \n",
    "    def set_opp_count(self):\n",
    "        self.opp_count = {a: 0 for a in self.rewards[self.select_action()].keys()}\n",
    "        \n",
    "    def record_opponent_action(self, opp_action):\n",
    "        if opp_action not in self.opp_count.keys():\n",
    "            raise KeyError(\"Opponent action different from the list of possible actions\")\n",
    "        self.opp_count[opp_action] += 1\n",
    "    \n",
    "    def select_action(self):\n",
    "        if self.best_response:\n",
    "            return self.select_best_response()\n",
    "        return choices(list(self.actions.keys()), weights=self.actions.values())[0]\n",
    "    \n",
    "    def select_best_response(self):\n",
    "        s = sum(self.opp_count.values())\n",
    "        utilities = {a: sum([self.rewards[a][a2] * c / s for a2, c in self.opp_count.items()])\n",
    "                     for a in self.actions.keys()}\n",
    "        return max(utilities, key=utilities.get)\n",
    "\n",
    "\n",
    "class Game:\n",
    "    def __init__(self, agents, n_initial_games=1, total_games=10000):\n",
    "        self.n_initial_games = n_initial_games\n",
    "        self.total_games = total_games\n",
    "        self.agents = agents\n",
    "    \n",
    "    def play_game(self):\n",
    "        a1, a2 = self.agents\n",
    "        a1.record_opponent_action(a2.select_action())\n",
    "        a2.record_opponent_action(a1.select_action())\n",
    "    \n",
    "    def play(self):\n",
    "        for agent in self.agents:\n",
    "            agent.set_opp_count()\n",
    "            agent.best_response = False\n",
    "        # Play some initial games randomly to accrue some data\n",
    "        for _ in range(self.n_initial_games):\n",
    "            self.play_game()\n",
    "        # Switch to playing best responses\n",
    "        for agent in self.agents:\n",
    "            agent.best_response = True\n",
    "        for _ in range(self.total_games - self.n_initial_games):\n",
    "            self.play_game()\n",
    "    \n",
    "    def summary(self):\n",
    "        print(f\"Player 1 plays the actions {tuple(game.agents[1].opp_count.keys())} with probabilities \" +\\\n",
    "              f\"{tuple(round(c / self.total_games, 3) for c in game.agents[1].opp_count.values())}\")\n",
    "        print(f\"Player 2 plays the actions {tuple(game.agents[0].opp_count.keys())} with probabilities \" +\\\n",
    "              f\"{tuple(round(c / self.total_games, 3) for c in game.agents[0].opp_count.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a94938b-f4dc-4ebd-ba82-fcabe6a2fde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1, A2 = ['A', 'B', 'C'], ['W', 'X', 'Y', 'Z']\n",
    "rewards = np.asarray([\n",
    "    [[1,5], [2,2], [3,4], [3,1]],\n",
    "    [[3,0], [4,1], [2,5], [4,2]],\n",
    "    [[1,3], [2,6], [5,2], [2,3]]\n",
    "])\n",
    "R1 = {a1: {a2: r[0] for a2, r in zip(A2, R)} for a1, R in zip(A1, rewards)}\n",
    "R2 = {a1: {a2: r[1] for a2, r in zip(A1, R)} for a1, R in zip(A2, rewards.swapaxes(0, 1))}\n",
    "game = Game([Agent(A1, R1), Agent(A2, R2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdb96bf-43f4-4699-9422-5b087e85ac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.play()\n",
    "game.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb869a9-1957-4129-be7b-b3fe4e324426",
   "metadata": {},
   "source": [
    "# Monte Carlo sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440c0acc-4538-4b07-ac00-96956e633b83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "size = 10000\n",
    "sample = np.cos(np.random.randn(size))**2\n",
    "print(f\"Expected value:\\t{sample.mean():.3}\\nVariance:\\t{sample.var():.3}\")\n",
    "\n",
    "# 99.9% confidence level is 3.291\n",
    "print(f\"The 99.9% confidence interval is {sample.mean():.3f}Â±{3.291*sample.std()/np.sqrt(size):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9054cf-a055-42bd-83a3-351a2e4088a7",
   "metadata": {},
   "source": [
    "# Thompson sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626fcfe7-38ca-4aa0-afd8-a1dd26b9478d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from random import uniform\n",
    "\n",
    "class KArmedBandit:\n",
    "    def __init__(self, k=1, probs=None):\n",
    "        self.arms = [uniform(0, 1) for _ in range(k)] if probs is None else [p for p in probs]\n",
    "    \n",
    "    def __call__(self, arm):\n",
    "        return self.pull(arm)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.arms[i]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        name = f\"{len(self.arms)}-Armed Bandit with:\\n\"\n",
    "        for i, p in enumerate(self.arms):\n",
    "            name += f\"  p{i+1}={p:.3f}\\n\"\n",
    "        return name[:-1]\n",
    "    \n",
    "    def pull(self, arm):\n",
    "        if type(arm) != int or len(self.arms) <= arm or arm < 0:\n",
    "            raise IndexError(\"Invalid arm selected\")\n",
    "        return int(self.arms[arm] > uniform(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a015942-1698-46b4-9b8d-46c128bdeea5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kab = KArmedBandit(probs=[0.2, 0.5, 0.9])\n",
    "samples = 1200\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.tight_layout()\n",
    "\n",
    "for arm in range(3):\n",
    "    rewards = np.asarray([kab(arm) for _ in range(samples)])\n",
    "    a, b = rewards.cumsum(), (1-rewards).cumsum()\n",
    "    mean = a/(a+b)\n",
    "    var = np.sqrt(a*b/((a+b)**2*(a+b+1)))\n",
    "\n",
    "    plt.plot(mean, label=f\"Arm with p={kab[arm]}\")\n",
    "    plt.fill_between(range(samples), mean + var, mean - var, alpha=0.6)\n",
    "plt.hlines(kab.arms, 0, samples, linestyle=\"--\", colors='dimgray')\n",
    "plt.legend()\n",
    "plt.yticks([0, 0.4, 0.6, 0.8, 1] + kab.arms)\n",
    "plt.title(\"Convergence to true probability\", fontsize=18)\n",
    "plt.ylabel(\"Probability p of getting reward r\", fontsize=14)\n",
    "plt.xlabel(\"Iteration\", fontsize=14)\n",
    "# plt.savefig(\"convergence.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4379ccdc-49ed-423a-9fc2-67be7ecc9cbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k = 3\n",
    "kab = KArmedBandit(k=k)\n",
    "params = np.asarray([[1, 1] for _ in range(k)])\n",
    "print(kab)\n",
    "\n",
    "regret = np.max(kab.arms) - np.asarray(kab.arms)\n",
    "arms = list()\n",
    "\n",
    "iterations = 1000 * k\n",
    "for i in range(iterations):\n",
    "    arm = np.argmax([np.random.beta(*ab) for ab in params])\n",
    "    r = kab(int(arm))\n",
    "    params[arm] += [r, 1-r]\n",
    "    arms.append(arm)\n",
    "    \n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.tight_layout()\n",
    "plt.plot(regret[arms].cumsum(), label=\"Thompson sampling\")\n",
    "\n",
    "for c, style in zip(np.logspace(-3, 0, 4), [(0, (1, 10)), '--', ':', '-.']):\n",
    "    counts, rewards = [0] * k, [0] * k\n",
    "    arms = list()\n",
    "    for i in range(iterations):\n",
    "        arm = np.argmax([np.infty if count == 0 else r/count + c*np.sqrt(np.log(i)/count)\n",
    "                         for r, count in zip(rewards, counts)])\n",
    "        r = kab(int(arm))\n",
    "        counts[arm] += 1\n",
    "        rewards[arm] += r\n",
    "        arms.append(arm)\n",
    "    plt.plot(regret[arms].cumsum(), label=f\"UCB with c={c:.0e}\", linestyle=style)\n",
    "plt.legend()\n",
    "plt.title(\"Thompson sampling vs UCB\", fontsize=18)\n",
    "plt.ylabel(\"Regret\", fontsize=14)\n",
    "plt.xlabel(\"Iteration\", fontsize=14)\n",
    "# plt.savefig(\"thompson_ucb.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
